# Random Forest-------------------------------------------------------------------------

# Always use set.seed: this will make the output reproducable
set.seed(84)
rf <-randomForest(df$outcome~.,data=df, ntree=500) 
print(rf)
View(rf)

# Selecting the best MTRY (how many features should be included on each node)
set.seed(84)
str(df)
mtry_df <- df[,-outcome]
str(mtry_df)
mtry <- tuneRF(mtry_df,df$outcome, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)

best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)

# Develop RandomForest with 10-fold Cross Validation

set.seed(84)
rf <- train(outcome ~ features,
                  data = df.train,
                  method = 'rf',
                  trControl = trainControl(method = "repeatedcv",
                                           number = 10,
                                           savePredictions = "final",
                                           classProbs = T),
                  tuneLength = 1,
                  importance = TRUE)


print(rf)
preds <- predict(rf, df.test)
confusionMatrix(preds, df.test$outcome)

#AUC
y.test = df.test$outcome
auc <- prediction(as.numeric(preds), as.numeric(y.test))
round(performance(auc, measure = "auc")@y.values[[1]],3)


#accuracy         
#sens              
#spec             
#precision (PPV)     
#AUC        

# Feauture engineering: plot feature importance

varimp.rf <- varImp(rf)
varimp.rf.rownames <- rf$importance
varimp.rf.rownames <- rownames(varimp.rf.rownames)

df.varimp.rf <- varimp.rf %>% 
  arrange(desc(varimp.rf$Overall))


x <- varimp.rf.rownames
x
y <- df.varimp.rf$Overall
y

plot.varimp.rf <- ggplot(data = varimp.rf, aes(reorder(x,y), y = y)) + 
  geom_bar(stat = "identity", aes(fill = Overall)) + 
  coord_flip() + 
  xlab("Features") + 
  ylab("Importance") +
  ggtitle("Importance for each feature") +
  labs(fill = "Value") +
  theme_classic()

plot(plot.varimp.rf)
